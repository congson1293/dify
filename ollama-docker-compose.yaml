services:
  ollama:
    image: repo.vps.com.vn/ollama/ollama:0.3.5
    container_name: ollama
    volumes:
      - ./resources:/root/llm_run
      - ollama:/root/.ollama

    ports:
      - "11434:11434"
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    entrypoint: ["/bin/sh", "-c", "OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_PARALLEL=3 OLLAMA_MAX_LOADED_MODELS=1 OLLAMA_FLASH_ATTENTION=1 OLLAMA_KEEP_ALIVE=10 OLLAMA_DEBUG=1 OLLAMA_ORIGINS=* ollama serve >> /root/llm_run/ollama.log 2>&1 & ollama create vps-code-copilot -f /root/llm_run/Modelfile_gemma2 && ollama create vps-code-copilot-embedder -f /root/llm_run/Modelfile_allminiLMv2 && tail -f /dev/null"]

volumes:
  ollama:
